<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
<script>
    (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/cat_track_32px.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/cat_track_16px.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>


  <meta name="description" content="Basic Machine Learning field of study that gives computers the ability to learn without being explicitly programmed. In other words, whatever the purpose of our algorithm is, the rules to achieve this">
<meta property="og:type" content="article">
<meta property="og:title" content="A gentle journey from linear regression to neural networks (1)">
<meta property="og:url" content="http://yoursite.com/2020/07/02/A%20gentle%20journey%20from%20linear%20regression%20to%20neural%20networks%20(1)/index.html">
<meta property="og:site_name" content="Bright&#39;s Blog">
<meta property="og:description" content="Basic Machine Learning field of study that gives computers the ability to learn without being explicitly programmed. In other words, whatever the purpose of our algorithm is, the rules to achieve this">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/Liangda-w/images/master/Quadratic%20minimisation%20with%20linear%20derivative.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/Liangda-w/images/master/Convex%20minimisation.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Liangda-w/images/master/Non-convex%20minimisation.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Liangda-w/images/master/underfitting%2C%20fitting%20and%20overfitting.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Liangda-w/images/master/fitting%20of%20a%205%20degrees%20polynomial%20function%20with%2010%2C%2030%20and%20100%20points.png">
<meta property="article:published_time" content="2020-07-02T16:28:54.000Z">
<meta property="article:modified_time" content="2020-07-03T14:14:50.511Z">
<meta property="article:author" content="大达哒 Bright">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Linear Regression">
<meta property="article:tag" content="encrypt">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Liangda-w/images/master/Quadratic%20minimisation%20with%20linear%20derivative.jpg">

<link rel="canonical" href="http://yoursite.com/2020/07/02/A%20gentle%20journey%20from%20linear%20regression%20to%20neural%20networks%20(1)/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>A gentle journey from linear regression to neural networks (1) | Bright's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
	<a href="https://github.com/Liangda-w/Liangda-w.github.io" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Bright's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">大达哒的小野望</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/02/A%20gentle%20journey%20from%20linear%20regression%20to%20neural%20networks%20(1)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="大达哒 Bright">
      <meta itemprop="description" content="书写是更好的思考, 讨论是绝佳的反思">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bright's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A gentle journey from linear regression to neural networks (1)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-02 18:28:54" itemprop="dateCreated datePublished" datetime="2020-07-02T18:28:54+02:00">2020-07-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-03 16:14:50" itemprop="dateModified" datetime="2020-07-03T16:14:50+02:00">2020-07-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/07/02/A%20gentle%20journey%20from%20linear%20regression%20to%20neural%20networks%20(1)/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/07/02/A%20gentle%20journey%20from%20linear%20regression%20to%20neural%20networks%20(1)/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>10 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Basic-Machine-Learning"><a href="#Basic-Machine-Learning" class="headerlink" title="Basic Machine Learning"></a>Basic Machine Learning</h2><p> field of study that gives computers the ability to learn without being explicitly programmed. In other words, whatever the purpose of our algorithm is, the rules to achieve this goal are not explicitly programmed but are <strong>“learned”</strong> by the computer, based on some useful data. </p>
<p>This is the big difference with a classical algorithm. In a classical algorithm, rules are explicitly given to the computer to perform a task. </p>
<p>In a machine learning algorithm, <strong>a model (parametrized or not) that define a family of possible rules is given to the computer along with a whole bunch of data and a strategy to find the better rules among the possible ones based on these data (optimisation).</strong></p>
<a id="more"></a>

<h3 id="linear-regression-线性回归"><a href="#linear-regression-线性回归" class="headerlink" title="linear regression 线性回归"></a>linear regression 线性回归</h3><p>Linear regression is one of the most simple examples of machine learning algorithms we can think of. Let us see that it perfectly fit the description we gave above. </p>
<p>Suppose that you have a bunch of data about houses on two attributes : </p>
<ul>
<li>size (s) </li>
<li>price (p). </li>
</ul>
<p>Now, suppose that you want a program that takes as an argument the size of a house and returns a price which is the estimated price of such house. </p>
<ol>
<li><p>The first option is to program explicitly the rule. In this case it means that we have to define explicitly the <strong>function f</strong> such that <strong>p = f(s)</strong>. In other words we have to give explicitly the price as a well defined function of the size. However we could have no idea of what this function can be or maybe we could have just a vague idea of it. </p>
</li>
<li><p>If so, we can rely on data to build our rule in a ML way. Then, we first define <strong>a set (a family) of rules</strong> : in this example we suppose that <strong>a linear rule</strong> expresses the link between price and size. So, we now have that f has the form <strong>f(s)=as+b</strong>, with a and b unspecified parameters (degrees of freedom) that need to be defined <strong>based on the available data and following a given strategy</strong>. Classically, for linear regression this strategy is very simple and consists to chose a and b such as to <strong>minimize the sum of squared errors between true outputs and predicted outputs</strong>. This can be done analytically in the linear regression case (we can find a closed form solution). But we will see that things are not always as easy. However we can notice that we have in this example our three mentioned parts : </p>
<ul>
<li><strong>a (parametrised) model</strong></li>
<li><strong>some data</strong></li>
<li><strong>an optimisation strategy (a way to find the optimal parameters)</strong></li>
</ul>
</li>
</ol>
<h3 id="Parametrised-vs-Non-parametrised-models"><a href="#Parametrised-vs-Non-parametrised-models" class="headerlink" title="Parametrised vs Non-parametrised models"></a>Parametrised vs Non-parametrised models</h3><p>The previous example of linear regression is an example of <strong>parametrised model</strong>, where a and b are the parameters. Along this document we will mainly deal with this kind of models because we want to show the transition up to <strong>neural networks</strong> which are <strong>(highly) parametrised</strong>. However one should keeps in mind that there also exist <strong>non-parametrised models</strong>. </p>
<p>Nevertheless, the triptych stays exactly the same. The model still defines a set of possible <strong>functions</strong> and the selection among this set is done based on <strong>available data</strong> following a given <strong>optimisation strategy</strong> (most of the time the <strong>minimisation of an error</strong>).</p>
<h2 id="Advanced-Machine-Learning"><a href="#Advanced-Machine-Learning" class="headerlink" title="Advanced Machine Learning"></a>Advanced Machine Learning</h2><h3 id="Problems-often-get-harder"><a href="#Problems-often-get-harder" class="headerlink" title="Problems often get harder"></a>Problems often get harder</h3><p>Of course problems can’t always be tackled with a method as simple as linear regression and, in most of the situations, we will have to build more complex models. Some problems do not even fit the specific framework suggested by linear regression (that is : take some real input and return a real output). Nevertheless, whatever the method we choose, we can always recover our underlying <strong>triptych model/data/optimisation</strong>.</p>
<ul>
<li><p>For example, consider that you have some data about the uses that your customers are doing of one of your services and you want a tool to predict if a client is going to churn or not based on these data. In this example the output to predict is not a real value but a <strong>binary value (two classes problem)</strong> and, so, we face a <strong>classification problem instead of a regression problem</strong>. Then, the output prediction should be a churn probability between 0 and 1 and the linear regression that has unbounded output should be, for example, turned into a <strong>logistic regression 逻辑回归</strong> by applying a <strong>dedicated non-linear function on top of the linear output</strong>.</p>
</li>
<li><p>Another example is to consider a customer segmentation problem. Suppose that you have some data about customers and you want to exhibit <strong>clusters</strong> in these data in order to obtain a customer segmentation. Here the problem is <strong>no more a supervised problem</strong>. Indeed, we do not want to learn <strong>a mapping between some inputs and a given output (supervised)</strong> but we look for <strong>some structures among unlabelled data (unsupervised)</strong>.</p>
</li>
</ul>
<p>Even if we are not going to give details about each and every kind of machine learning algorithms, we can give the following big picture : <strong>more advanced models are designed to express more complex structures in the data but it can be at the price of some difficulties to optimise or to interpret the model.</strong> Let us discuss these two points thereafter.</p>
<h3 id="Optimisation"><a href="#Optimisation" class="headerlink" title="Optimisation"></a>Optimisation</h3><p>Once the model defined, it has to be <strong>optimised to fit the data “just enough” to capture the relevant general structures in the data</strong> while <strong>letting aside irrelevant specific</strong> <strong>“noise”</strong> (we will discuss later the notion of <strong>“overfitting”</strong>). </p>
<p>Indeed, as it was previously mentioned, a model define a <strong>space</strong> (of possible “instance” of this model) and we need to find the optimal point in this space with respect to a chosen <strong>metric</strong> (a way to evaluate the quality of each point in that space). </p>
<ul>
<li>This metric is most often defined by <strong>an error term</strong>, that <strong>penalise处罚</strong> instance of the model that doesn’t fit the data enough, </li>
<li>and sometimes joined by a <strong>regularisation term</strong>, that <strong>penalise</strong> instance of the model that are too <strong>complex</strong> and too <strong>close</strong> to the data. </li>
</ul>
<p>Then, we can mainly classify the optimisation problem that we get in three categories.</p>
<ol>
<li><p>First of all, for very simple methods, we can face a <strong>quadratic-like optimisation problem</strong>. Here we mean cases where it is possible to find a closed-form solution to our problem (we know how to mathematically express the solution of the problem). <strong>Linear regression</strong> is clearly such a method. The fact that we can obtain a closed-form solution is very appealing but also reflect the <strong>simplicity</strong>, and so the <strong>inability to catch complex structures</strong> in the data, of the space defined by the model.<br>首先，对于非常简单的方法，我们可以面对二次方优化问题。 在这里，我们指的是有可能找到我们问题的封闭式解决方案的情况（我们知道如何以数学方式表达问题的解决方案）。 线性回归显然就是这种方法。 我们可以获得封闭形式的解决方案这一事实非常吸引人，但也反映出模型定义的空间的简单性，因此无法捕获数据中的复杂结构。</p>
<p> <img src="https://raw.githubusercontent.com/Liangda-w/images/master/Quadratic%20minimisation%20with%20linear%20derivative.jpg" alt=""></p>
<p> <em>Quadratic minimisation with linear derivative. Finding the minimum is straightforward in an analytical way : we just need to find the 0 of a linear function.</em></p>
</li>
<li><p>Then, the optimisation problem can be non-quadratic but <strong>convex</strong>. <strong>Non-quadratic optimisation</strong> problems often can’t be solved in an analytical way and require most of the time an <strong>iterative approach</strong>. </p>
<p> The main idea behind these iterative approaches is to start at a given point of the space — the space described by our model, in which a point is an instance of the model, for example with specific parameters — and to try to improve this solution iteration by iteration choosing, at each iteration, to make a little step in the best possible direction in our space (depending on how we define the notion of “best”). These iterative approaches can take different shapes such as various kinds of <strong>gradient descents variants 梯度下降变体</strong>, <strong>EM algorithms</strong> and others, but at the end the underlying idea is the same : <strong>we can’t find direct solution so we start from a given point and progress step by step taking at each iteration a little step in a direction that improve our current solution</strong>. See the next figure for an illustration of gradient descent. For such iterative methods, the <strong>convexity</strong> of the space is a very important property that ensure we will reach the global optimum no matter the chosen starting point (the instance of the model we will obtain will be the best possible with respect to the defined optimisation problem).</p>
<p> <img src="https://raw.githubusercontent.com/Liangda-w/images/master/Convex%20minimisation.png" alt=""></p>
<p> <em>Convex minimisation. Finding the minimum is not straightforward and require iterative approach (as the derivative is no longer linear). Here, we used gradient descent. However, convexity ensure that iterative approach will reach the global minimum.</em></p>
</li>
<li><p>Finally, the space defined by the model can be non-convex and we then face a <strong>non-convex optimisation problem</strong>. In this case, the non-linearity will again impose to make use of an iterative approach. However the non-convexity makes that we are not sure anymore that the iterative procedure will reach the best possible point over the entire space. In other words the <strong>optimum</strong> (the instance of the model) we obtain highly <strong>depends on the starting point</strong> we choose. This is obviously the worst case of optimisation problem we can face and an obvious drawback of the high ability of some more advanced models to express complex structures inside the data.</p>
<p> <img src="https://raw.githubusercontent.com/Liangda-w/images/master/Non-convex%20minimisation.png" alt=""></p>
<p> <em>Non-convex minimisation. Finding the minimum require iterative approach (here, gradient descent) but the non-convexity make it possible to reach local minima instead of the global minimum depending on the starting point.</em></p>
</li>
</ol>
<h3 id="Be-careful-about-overfitting"><a href="#Be-careful-about-overfitting" class="headerlink" title="Be careful about overfitting"></a>Be careful about overfitting</h3><p>In machine learning, in general, when dealing with the optimisation process, we need to be extremely careful about <strong>overfitting</strong>. We say that the model overfits the data when the model has learned not only the interesting general features that we desire but also some specific <strong>undesired noise</strong>. In other words, overfitting is when the optimisation process lead to an instance of model that is <strong>too close to the training data</strong> and, so, that won’t generalise very well to new unseen data. The following figure illustrates pretty well the phenomenon of overfitting.</p>
<p><img src="https://raw.githubusercontent.com/Liangda-w/images/master/underfitting%2C%20fitting%20and%20overfitting.png" alt=""></p>
<p><em>From left to right : <strong>underfitting, fitting and overfitting</strong>. Over the same set of data we fit polynomial functions of various degrees. We can see that 1 degree polynomial function has not enough freedom while 20 degrees polynomial function has too much.</em></p>
<p>The ratio between <strong>the number of parameters</strong> of the model and the <strong>number of data available</strong> for the training has an impact on the overfitting risk. </p>
<ul>
<li>If there are not enough data compared to the number of parameters, there is, in some sense, space for learning undesired noise. </li>
<li>But if the number of data is great enough, it will have a <strong>regularisation effect</strong> and will force the parameters to learn general features only (see the following figure). </li>
<li>When fitting a neural network, the number of parameters can be very high and so is the risk of overfitting.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Liangda-w/images/master/fitting%20of%20a%205%20degrees%20polynomial%20function%20with%2010%2C%2030%20and%20100%20points.png" alt=""></p>
<p><em>From left to right : fitting of a 5 degrees polynomial function with 10, 30 and 100 points. We can see that <strong>more points have a regularisation effect</strong> : with 100 points, the fitted curve looks like a 2 degrees polynomial function.</em></p>
<h3 id="Model-interpretability"><a href="#Model-interpretability" class="headerlink" title="Model interpretability"></a>Model interpretability</h3><p>The <strong>interpretability</strong> of a model is a big question that can’t be neglected and is sometimes a determining factor when deciding to choose a model rather than another.By “<strong>interpretability</strong>” of the model we mean the ability, once the model optimised, to understand <strong>why some inputs give some outputs</strong>. </p>
<p>Let us take the example of the <strong>linear regression</strong> of the previous section <strong>(p=f(s)=as+b</strong>, <strong>with s the size of a house and p its price)</strong>. Once we have optimised parameters a and b, based on the data, we can fully interpret what we obtained. Indeed we can say, in order to guess the price of a new house, that we have a base price of b units of money to which we add a units of money for each unit of size of the house. Here again, the <strong>full interpretability of linear regression comes from its over-simplicity</strong> and along with some limitations in terms of modeling. </p>
<p>At the opposite, some models are very powerful but far less (if any) interpretable. However, there is not necessarily a direct link between performance of a model and its interpretability. SVMs are, for example, known to perform well on many problems and their interpretability can be also pretty good.</p>
<h3 id="Importance-of-interpretability"><a href="#Importance-of-interpretability" class="headerlink" title="Importance of interpretability"></a>Importance of interpretability</h3><p>Notice that, with the more and more important place that Machine Learning is going to take in our daily lives, the question of interpretability is becoming central and will be more and more in the future. Machine Learning models are going to assist humans for some (possibly important) tasks (in health, finance, driving…) and we sometimes want to be able to understand how the results returned by the models are obtained. </p>
<ul>
<li>For example, a smart keyboard that suggest next most probable words when typing a message doesn’t necessarily need to be understandable : we just want it to be efficient. </li>
<li>However, a model that predict the presence of a disease or not for a patient better be interpretable on top of accurate : in this case we are not only interested by the result but we want to understand the “logic” behind it in order to let a human confirm, or not, the diagnosis.</li>
</ul>
<p>So, interpretability is a very appealing feature for a model. However, sometimes it’s necessary to trade some (if not all) interpretability for a greater predictive power by setting up some very complex models : that is exactly what is done in the case of neural networks discussed in the following section.</p>
<hr>
<p>Reference：<a href="https://towardsdatascience.com/a-gentle-journey-from-linear-regression-to-neural-networks-68881590760e">A gentle journey from linear regression to neural networks, Joseph Rocca, Dec 8, 2018</a></p>

    </div>

    
    
    

    
     <div>
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:18px;">-------------End <i class="fa fa-paw"></i> Thank you for reading-------------</div>
    
</div>
     </div>
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>大达哒 Bright
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="http://yoursite.com/2020/07/02/A%20gentle%20journey%20from%20linear%20regression%20to%20neural%20networks%20(1)/" title="A gentle journey from linear regression to neural networks (1)">http://yoursite.com/2020/07/02/A gentle journey from linear regression to neural networks (1)/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Linear-Regression/" rel="tag"># Linear Regression</a>
              <a href="/tags/encrypt/" rel="tag"># encrypt</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/06/25/Leetcode-287/" rel="prev" title="Leetcode 287： Find the Duplicate Number">
      <i class="fa fa-chevron-left"></i> Leetcode 287： Find the Duplicate Number
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-Machine-Learning"><span class="nav-number">1.</span> <span class="nav-text">Basic Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#linear-regression-线性回归"><span class="nav-number">1.1.</span> <span class="nav-text">linear regression 线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parametrised-vs-Non-parametrised-models"><span class="nav-number">1.2.</span> <span class="nav-text">Parametrised vs Non-parametrised models</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Advanced-Machine-Learning"><span class="nav-number">2.</span> <span class="nav-text">Advanced Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Problems-often-get-harder"><span class="nav-number">2.1.</span> <span class="nav-text">Problems often get harder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimisation"><span class="nav-number">2.2.</span> <span class="nav-text">Optimisation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Be-careful-about-overfitting"><span class="nav-number">2.3.</span> <span class="nav-text">Be careful about overfitting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-interpretability"><span class="nav-number">2.4.</span> <span class="nav-text">Model interpretability</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Importance-of-interpretability"><span class="nav-number">2.5.</span> <span class="nav-text">Importance of interpretability</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="大达哒 Bright"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">大达哒 Bright</p>
  <div class="site-description" itemprop="description">书写是更好的思考, 讨论是绝佳的反思</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Liangda-w" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Liangda-w" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liangda_wang@outlook.de" title="E-Mail → mailto:liangda_wang@outlook.de" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020.06.17 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">大达哒 Bright</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">27k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">25 mins.</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv"> | 
	<span class="post-meta-item-icon">
      		<i class="fa fa-eye"></i>
     	</span>
    <span id="busuanzi_value_site_pv"></span></span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv"><span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span> <span id="busuanzi_value_site_uv"></span></span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'BKmvvq7ui9KM19jjwxYLOROu-MdYXbMMI',
      appKey     : 'l07Jlh3vsQcz1hhASKXNNY8z',
      placeholder: "Your comment here:)",
      avatar     : 'wavatar',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
